{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Convolutional Neural Network with hyperparameter optimization\n",
    "\n",
    "We're going to train a full, regularized CNN architecture with some automatic hyperperameter optimization using `hyperas` (a wrapper for `hyperopt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set\n",
      "Count of each class\n",
      "Classes\n",
      "0.0    72471\n",
      "1.0     2223\n",
      "2.0     5788\n",
      "3.0      641\n",
      "4.0     6431\n",
      "dtype: int64\n",
      "\n",
      "Test set\n",
      "Count of each class\n",
      "Classes\n",
      "0.0    18118\n",
      "1.0      556\n",
      "2.0     1448\n",
      "3.0      162\n",
      "4.0     1608\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "import tools.train as train\n",
    "import tools.models as models\n",
    "\n",
    "## Read in data\n",
    "files = (\"../data/mitbih_train.csv\", \"../data/mitbih_test.csv\")\n",
    "inputs, labels, sparse_labels, df = train.preprocess(*files, fft=False)\n",
    "# Add a dimension for \"channels\"\n",
    "for key in inputs:\n",
    "    inputs[key] = tf.expand_dims(inputs[key], axis=2)\n",
    "train.class_count(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress tensorflow warnings about internal deprecations\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "# functions for hyperas\n",
    "def data():\n",
    "    return inputs[\"train\"], labels[\"train\"], inputs[\"test\"], labels[\"test\"]\n",
    "\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    nblocks = {{choice([4, 8])}}\n",
    "    nfilters = 32\n",
    "    dilation_limit = inputs[\"train\"].shape[1]\n",
    "    \n",
    "    # Start\n",
    "    layerlist_res = [(\"conv\", {\"filters\": nfilters, \"width\": 1, \"padding\": \"causal\"})]\n",
    "\n",
    "    # Residual blocks\n",
    "    models.add_res_blocks(nblocks, nfilters, dilation_limit, layerlist_res)\n",
    "\n",
    "    # End\n",
    "    layerlist_res.extend([\n",
    "        (layers.Activation(\"relu\"),),\n",
    "        (\"conv\", {\"filters\": nfilters, \"width\": 1, \"padding\": \"causal\"}),\n",
    "        (\"conv\", {\"filters\": 1, \"width\": 1, \"padding\": \"causal\"}),\n",
    "        (layers.Dropout({{uniform(0, 1)}},),)\n",
    "    ])\n",
    "\n",
    "    config = {\"optimizer\": {{choice([\"Nadam\", \"sgd\"])}},\n",
    "              \"loss\": \"categorical_crossentropy\",\n",
    "              \"batch_size\": {{choice([50, 100, 200])}},\n",
    "              \"val_split\": 0.05,\n",
    "              \"epochs\": 300,\n",
    "              \"verbose\": 0,\n",
    "              \"patience\": 20}\n",
    "\n",
    "    inputsize = inputs[\"train\"].shape[1]\n",
    "    ncategories = labels[\"train\"].shape[1]\n",
    "    model_res = models.create_conv1d(inputsize, layerlist_res, ncategories)\n",
    "    history = train.train(model_res, inputs, labels, config)\n",
    "    \n",
    "    # get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(history.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model_res}\n",
    "\n",
    "best_run, best_model = optim.minimize(\n",
    "    model=create_model,\n",
    "    data=data,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5,\n",
    "    trials=Trials()\n",
    ")\n",
    "\n",
    "print(\"Chosen hyperparameters from the best-trained model\")\n",
    "print(best_run)\n",
    "\n",
    "print(\n",
    "    \"Train acc of best performing model:\",\n",
    "    model.evaluate(inputs[\"train\"], labels[\"train\"], verbose=0)[1],\n",
    ")\n",
    "print(\n",
    "    \"Test acc of best performing model:\",\n",
    "    model.evaluate(inputs[\"test\"], labels[\"test\"], verbose=0)[1],\n",
    "\n",
    "test_pred = np.argmax(model_res.predict(inputs[\"test\"]), axis=1)\n",
    "plot.plot_cm(sparse_labels[\"test\"], test_pred, classes=[\"N\", \"S\", \"V\", \"F\", \"Q\"], normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
